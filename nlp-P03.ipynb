{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fff3f13-9ffb-4087-978c-9107c429c886",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf2fd4-9fad-4b6d-89b0-e3adbcfa1855",
   "metadata": {},
   "source": [
    "Instruction:  \n",
    "Please also make sure to structure your notebooks as if you were conducting this as a clean and nicely presented data analysis report. Do not include our prompts/problem statements in the final report notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3439960e-877d-488a-8e67-4ef6c8cdc6ec",
   "metadata": {},
   "source": [
    "In this section, we apply two types of topic modeling methods —\n",
    "(1) LDA (Latent Dirichlet Allocation) and (2) BERTopic — to the State of the Union (SOTU) corpus.\n",
    "We use lemmas as the base representation and compare how traditional bag-of-words models differ from transformer-based semantic models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61428210-6584-4bcc-9a91-5cfca341f743",
   "metadata": {},
   "source": [
    "## 3.1 Preprocessing for Topic Modeling\n",
    "\n",
    "We load the dataset, apply spaCy to tokenize and lemmatize text, and construct a list of lemma tokens for each document. Stop words, punctuation, and whitespace tokens are removed to prepare clean inputs for topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f645c467-2ead-4368-9eb9-973301a98d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts for LDA: 100%|██████████| 246/246 [04:38<00:00,  1.13s/it]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "sou = pd.read_csv(\"data/SOTU.csv\")\n",
    "raw_docs = sou[\"Text\"].tolist()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "processed_docs = []\n",
    "for text in tqdm(raw_docs, desc=\"Processing texts for LDA\"):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [\n",
    "        token.lemma_.lower()\n",
    "        for token in doc\n",
    "        if not token.is_stop and not token.is_punct and not token.is_space\n",
    "    ]\n",
    "    processed_docs.append(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d6f9be-b0e7-4204-808e-581cf6a75933",
   "metadata": {},
   "source": [
    "## 3.2 LDA Topic Modeling\n",
    "\n",
    "We construct a dictionary and bag-of-words corpus from the lemma lists.\n",
    "Using Gensim’s LDA model, we extract a set of topics from the entire SOTU speech collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c07d2b09-546d-4002-b8c8-b382039a35ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- LDA Topic 0 ---\n",
      "(0, '0.010*\"government\" + 0.009*\"year\" + 0.007*\"world\" + 0.007*\"nation\" + 0.007*\"program\" + 0.006*\"congress\" + 0.006*\"people\" + 0.006*\"war\" + 0.006*\"federal\" + 0.005*\"national\"')\n",
      "--- LDA Topic 1 ---\n",
      "(1, '0.013*\"year\" + 0.009*\"america\" + 0.009*\"people\" + 0.008*\"new\" + 0.007*\"world\" + 0.007*\"work\" + 0.007*\"american\" + 0.006*\"nation\" + 0.005*\"congress\" + 0.005*\"help\"')\n",
      "--- LDA Topic 2 ---\n",
      "(2, '0.011*\"government\" + 0.009*\"states\" + 0.009*\"year\" + 0.008*\"$\" + 0.008*\"united\" + 0.005*\"congress\" + 0.005*\"country\" + 0.005*\"law\" + 0.004*\"great\" + 0.004*\"public\"')\n",
      "--- LDA Topic 3 ---\n",
      "(3, '0.012*\"states\" + 0.012*\"government\" + 0.008*\"united\" + 0.007*\"congress\" + 0.007*\"country\" + 0.006*\"year\" + 0.006*\"great\" + 0.005*\"public\" + 0.005*\"$\" + 0.005*\"law\"')\n",
      "--- LDA Topic 4 ---\n",
      "(4, '0.008*\"government\" + 0.008*\"man\" + 0.008*\"law\" + 0.007*\"great\" + 0.006*\"country\" + 0.005*\"nation\" + 0.005*\"people\" + 0.005*\"work\" + 0.005*\"congress\" + 0.004*\"need\"')\n",
      "--- LDA Topic 5 ---\n",
      "(5, '0.006*\"government\" + 0.004*\"measure\" + 0.004*\"law\" + 0.004*\"public\" + 0.004*\"states\" + 0.004*\"provision\" + 0.004*\"united\" + 0.003*\"country\" + 0.003*\"citizen\" + 0.003*\"present\"')\n",
      "--- LDA Topic 6 ---\n",
      "(6, '0.003*\"states\" + 0.003*\"year\" + 0.003*\"wool\" + 0.003*\"government\" + 0.002*\"congress\" + 0.002*\"great\" + 0.002*\"$\" + 0.002*\"people\" + 0.002*\"american\" + 0.002*\"united\"')\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_docs)\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
    "\n",
    "\n",
    "lda = LdaModel(\n",
    "    corpus=corpus, \n",
    "    id2word=dictionary, \n",
    "    num_topics=7,   \n",
    "    passes=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "lda_topics = lda.print_topics(num_words=10)\n",
    "for idx, topic in enumerate(lda_topics):\n",
    "    print(f\"--- LDA Topic {idx} ---\")\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c61fdc5-792f-40fe-8e8a-00878588078c",
   "metadata": {},
   "source": [
    "## LDA Topic Interpretation\n",
    "\n",
    "The LDA topics generally capture high-frequency, surface-level themes in the corpus.\n",
    "Common words such as government, year, world, nation, states, congress appear across many topics, showing that LDA groups speeches based on repeated vocabulary patterns rather than semantic meaning.\n",
    "Because SOTU speeches share strong stylistic overlap, the LDA topics tend to be broad and partially overlapping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f054ff4-62ff-4168-aefb-058d1ae80e4d",
   "metadata": {},
   "source": [
    "## 3.3 BERTopic Modeling\n",
    "\n",
    "We further clean the lemma lists by removing additional English stopwords from NLTK, then apply BERTopic, which uses transformer embeddings and density-based clustering to form semantically coherent clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec957135-99c5-4cb1-89fc-d4d41f55d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "clean_docs = []\n",
    "for doc in processed_docs:\n",
    "    filtered = [w for w in doc if w not in stop_words]\n",
    "    clean_docs.append(\" \".join(filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49a0c54c-39b9-4fb2-ab04-63635ce159b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>0_government_states_united_congress</td>\n",
       "      <td>[government, states, united, congress, year, c...</td>\n",
       "      <td>[congress united states discharge constitution...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>91</td>\n",
       "      <td>1_year_america_people_world</td>\n",
       "      <td>[year, america, people, world, new, nation, am...</td>\n",
       "      <td>[everybody seat mr. speaker mr. vice president...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                 Name  \\\n",
       "0      0    155  0_government_states_united_congress   \n",
       "1      1     91          1_year_america_people_world   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [government, states, united, congress, year, c...   \n",
       "1  [year, america, people, world, new, nation, am...   \n",
       "\n",
       "                                 Representative_Docs  \n",
       "0  [congress united states discharge constitution...  \n",
       "1  [everybody seat mr. speaker mr. vice president...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probs = topic_model.fit_transform(clean_docs)\n",
    "\n",
    "\n",
    "topic_info = topic_model.get_topic_info()\n",
    "topic_info.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a53974-b8c6-45de-a7ce-6e6da1960fb5",
   "metadata": {},
   "source": [
    "## BERTopic Topic Interpretation\n",
    "\n",
    "BERTopic typically produces fewer, more semantically distinct topics.\n",
    "Because transformer embeddings capture contextual meaning, the model groups speeches into broader conceptual clusters such as:\n",
    "\n",
    "- Government/Congress/National Affairs\n",
    "\n",
    "- America/People/World/Future Themes\n",
    "\n",
    "This reflects semantic coherence, contrasting with LDA’s frequency-driven clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfac3664-8622-4667-8dda-b15693a6cccb",
   "metadata": {},
   "source": [
    "## 3.4 Comparison: LDA vs BERTopic\n",
    "\n",
    "- LDA uses a bag-of-words representation and identifies topics based on word co-occurrence.\n",
    "   <br>→ Produces several overlapping topics with similar vocabulary.\n",
    "\n",
    "- BERTopic uses sentence-level semantic embeddings from transformers.\n",
    "   <br>→ Produces fewer, but more meaningful and interpretable clusters.\n",
    "\n",
    "- Since SOTU speeches share structure and vocabulary,\n",
    "   <br>→ LDA tends to fragment content into many similar topics,\n",
    "   <br>→ BERTopic groups documents by broader themes reflecting actual meaning.\n",
    "\n",
    "Overall, BERTopic provides more coherent high-level topics, while LDA gives granular but noisy lexical clusters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
