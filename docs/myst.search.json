{"version":"1","records":[{"hierarchy":{"lvl1":"Project 02 — Reproducibility in Modern NLP"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"Project 02 — Reproducibility in Modern NLP"},"content":"","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"Project 02 — Reproducibility in Modern NLP","lvl2":"Group 23 — Statistics 159/259 (Fall 2025)"},"type":"lvl2","url":"/#group-23-statistics-159-259-fall-2025","position":2},{"hierarchy":{"lvl1":"Project 02 — Reproducibility in Modern NLP","lvl2":"Group 23 — Statistics 159/259 (Fall 2025)"},"content":"Welcome!This site contains our reproducible NLP analysis project across four notebooks.\n\nYou can also run this project interactively in Binder:\n\n[","type":"content","url":"/#group-23-statistics-159-259-fall-2025","position":3},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)"},"type":"lvl1","url":"/nlp-p01","position":0},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)"},"content":"In this section, we import the data and perform EDA.Before starting the analysis, we first created the conda environment with the provided yaml file in the terminal. (conda env create -f environment.yml)\n\n","type":"content","url":"/nlp-p01","position":1},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"Basic Imports and Plot Settings"},"type":"lvl2","url":"/nlp-p01#basic-imports-and-plot-settings","position":2},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"Basic Imports and Plot Settings"},"content":"\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.style.use('seaborn-v0_8-dark')\n\n","type":"content","url":"/nlp-p01#basic-imports-and-plot-settings","position":3},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.1 Read Data"},"type":"lvl2","url":"/nlp-p01#id-1-1-read-data","position":4},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.1 Read Data"},"content":"We load the dataset into a pandas dataframe.\n\nsou = pd.read_csv(\"data/SOTU.csv\")\nsou[\"Year\"] = sou[\"Year\"].astype(int)\n\nsou.head()\n\n","type":"content","url":"/nlp-p01#id-1-1-read-data","position":5},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl2","url":"/nlp-p01#id-1-2-exploratory-data-analysis","position":6},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.2 Exploratory Data Analysis"},"content":"\n\n","type":"content","url":"/nlp-p01#id-1-2-exploratory-data-analysis","position":7},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.1 Number of Speeches per President","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl3","url":"/nlp-p01#id-1-2-1-number-of-speeches-per-president","position":8},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.1 Number of Speeches per President","lvl2":"1.2 Exploratory Data Analysis"},"content":"Here, we count the number of speeches per president, reorder them to match the dataset’s original president order, and visualize the results with a bar chart.\n\nspeeches_per_pres = sou[\"President\"].value_counts()\n\npres_order = sou[\"President\"].drop_duplicates()\nspeeches_per_pres = speeches_per_pres.reindex(pres_order)\n\nspeeches_per_pres\n\nax = speeches_per_pres.plot(kind=\"bar\")\n\nax.set_title(\"Number of Speeches per President\")\nax.set_ylabel(\"Count\")\nplt.xticks(rotation=90)\nplt.tight_layout()\n\nplt.savefig(\"outputs/P01_figure_1.png\")\n\n","type":"content","url":"/nlp-p01#id-1-2-1-number-of-speeches-per-president","position":9},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.2 Number of Speeches per Year","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl3","url":"/nlp-p01#id-1-2-2-number-of-speeches-per-year","position":10},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.2 Number of Speeches per Year","lvl2":"1.2 Exploratory Data Analysis"},"content":"We count the number of speeches given each year, sort them in chronological order, and plot the yearly trend as a line chart.\n\nspeeches_per_year = sou[\"Year\"].value_counts().sort_index()\n\nax = speeches_per_year.plot()\nax.set_title(\"Number of State of the Union Speeches per Year\")\nax.set_xlabel(\"Year\")\nax.set_ylabel(\"Count\")\nplt.tight_layout()\n\nplt.savefig(\"outputs/P01_figure_2.png\")\n\n","type":"content","url":"/nlp-p01#id-1-2-2-number-of-speeches-per-year","position":11},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.3 Word Count Distribution","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl3","url":"/nlp-p01#id-1-2-3-word-count-distribution","position":12},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.3 Word Count Distribution","lvl2":"1.2 Exploratory Data Analysis"},"content":"We visualize the distribution of speech lengths by plotting a histogram of word counts across all State of the Union speeches.\n\nsns.histplot(data=sou, x=\"Word Count\", bins=18)\n\nplt.title(\"Distribution of State of the Union Speech Word Counts\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\n\nplt.savefig(\"outputs/P01_figure_3.png\")\n\n","type":"content","url":"/nlp-p01#id-1-2-3-word-count-distribution","position":13},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.4 Word Count Distribution over Year","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl3","url":"/nlp-p01#id-1-2-4-word-count-distribution-over-year","position":14},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.4 Word Count Distribution over Year","lvl2":"1.2 Exploratory Data Analysis"},"content":"We explore the relationship between speech length and time by plotting a scatterplot of word count vs. year, with rug plots added to show the marginal distributions along each axis.\n\nsns.scatterplot(data=sou, x=\"Word Count\", y=\"Year\")\n\nsns.rugplot(data=sou, x=\"Word Count\")\nsns.rugplot(data=sou, y=\"Year\", color=\"#1f77b4\") # changed color to match original plot\n\nplt.title(\"Speech Year Versus Word Count\")\nplt.xlabel(\"Word Count\")\nplt.ylabel(\"Year\")\nplt.tight_layout()\n\nplt.savefig(\"outputs/P01_figure_4.png\")\n\n","type":"content","url":"/nlp-p01#id-1-2-4-word-count-distribution-over-year","position":15},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.5 Word Count Distribution per President","lvl2":"1.2 Exploratory Data Analysis"},"type":"lvl3","url":"/nlp-p01#id-1-2-5-word-count-distribution-per-president","position":16},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl3":"1.2.5 Word Count Distribution per President","lvl2":"1.2 Exploratory Data Analysis"},"content":"We compute each president’s average speech length, reorder the results to match the dataset’s president order, and visualize the averages using a bar chart.\n\nplt.figure(figsize=(15, 6))\n\navg_words_pres = sou.groupby(\"President\")[\"Word Count\"].mean()\n\navg_words_pres = avg_words_pres.reindex(pres_order)\n\nax = avg_words_pres.plot(kind=\"bar\")\n\nax.set_title(\"Average State of the Union Word Count\\nper President\")\nax.set_ylabel(\"Average Word Count\")\nplt.xticks(fontsize=10, rotation=90)\nplt.tight_layout()\n\nplt.savefig(\"outputs/P01_figure_5.png\")\n\n","type":"content","url":"/nlp-p01#id-1-2-5-word-count-distribution-per-president","position":17},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.3 Commentary on My Findings"},"type":"lvl2","url":"/nlp-p01#id-1-3-commentary-on-my-findings","position":18},{"hierarchy":{"lvl1":"Part 1: Data Loading and Initial Exploration (15 pts)","lvl2":"1.3 Commentary on My Findings"},"content":"\n\n(write here)","type":"content","url":"/nlp-p01#id-1-3-commentary-on-my-findings","position":19},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"type":"lvl1","url":"/nlp-p02","position":0},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)"},"content":"In this section, we explored the most common tokens and lemmas throughout different slices of the speech data and developed vectorization representations of the speeches.\n\n","type":"content","url":"/nlp-p02","position":1},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Basic Imports"},"type":"lvl2","url":"/nlp-p02#basic-imports","position":2},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"Basic Imports"},"content":"\n\nimport spacy\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.colors import LogNorm\nfrom tqdm import tqdm\nimport numpy as np\nfrom collections import Counter\n\n","type":"content","url":"/nlp-p02#basic-imports","position":3},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.1 Processing Speeches with SpaCy"},"type":"lvl2","url":"/nlp-p02#id-2-1-processing-speeches-with-spacy","position":4},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.1 Processing Speeches with SpaCy"},"content":"\n\nnlp = spacy.load(\"en_core_web_sm\")\n\n# subset the speech dataframe for speeches from 2000 and onwards\nsou = pd.read_csv(\"data/SOTU.csv\")\nsou[\"Year\"] = sou[\"Year\"].astype(int)\nsou_2000 = sou[sou[\"Year\"] >= 2000]\n\n# Process speeches\nprocessed_speeches = []\nfor text in tqdm(sou_2000[\"Text\"], desc=\"Processing speeches\"):\n    processed_speeches.append(nlp(text))\n\n\n","type":"content","url":"/nlp-p02#id-2-1-processing-speeches-with-spacy","position":5},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.2 Analyze Tokens vs Lemmas"},"type":"lvl2","url":"/nlp-p02#id-2-2-analyze-tokens-vs-lemmas","position":6},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.2 Analyze Tokens vs Lemmas"},"content":"\n\n","type":"content","url":"/nlp-p02#id-2-2-analyze-tokens-vs-lemmas","position":7},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.1 Token List","lvl2":"2.2 Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#id-2-2-1-token-list","position":8},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.1 Token List","lvl2":"2.2 Analyze Tokens vs Lemmas"},"content":"\n\ntokens = []\n\nfor doc in processed_speeches:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            tokens.append(token.text.lower())\n\ntoken_counts = Counter(tokens)\ntoken_counts.most_common(25)\n\n","type":"content","url":"/nlp-p02#id-2-2-1-token-list","position":9},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.2 Lemma List","lvl2":"2.2 Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#id-2-2-2-lemma-list","position":10},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.2 Lemma List","lvl2":"2.2 Analyze Tokens vs Lemmas"},"content":"\n\nlemmas = []\n\nfor doc in processed_speeches:\n    for token in doc:\n        if not token.is_stop and not token.is_punct and not token.is_space:\n            lemmas.append(token.lemma_.lower())\n\nlemma_counts = Counter(lemmas)\nlemma_counts.most_common(25)\n\n","type":"content","url":"/nlp-p02#id-2-2-2-lemma-list","position":11},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.3 Token versus Lemma Comparison","lvl2":"2.2 Analyze Tokens vs Lemmas"},"type":"lvl3","url":"/nlp-p02#id-2-2-3-token-versus-lemma-comparison","position":12},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.2.3 Token versus Lemma Comparison","lvl2":"2.2 Analyze Tokens vs Lemmas"},"content":"\n\n(write your analysis here)What do you notice about the top tokens versus the top lemmas?\nConsider two tokens - “year” and “years” - how do their counts compare to the lemma “year”?\nWhat about the lemma “child”?\n\n","type":"content","url":"/nlp-p02#id-2-2-3-token-versus-lemma-comparison","position":13},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.3 Common Words"},"type":"lvl2","url":"/nlp-p02#id-2-3-common-words","position":14},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.3 Common Words"},"content":"\n\n","type":"content","url":"/nlp-p02#id-2-3-common-words","position":15},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.3.1 Common Words per Year Function","lvl2":"2.3 Common Words"},"type":"lvl3","url":"/nlp-p02#id-2-3-1-common-words-per-year-function","position":16},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.3.1 Common Words per Year Function","lvl2":"2.3 Common Words"},"content":"\n\ndef get_most_common_words(df, year, n=25):\n    \"\"\"\n    Processes the SOTU speeches for a given year and returns \n    the top n most common non-stopword, non-punctuation lemmas.\n    \"\"\"\n\n    df_year = df[df[\"Year\"] == int(year)]\n    \n    year_lemmas = []\n    for text in df_year[\"Text\"]:\n        doc = nlp(text)\n        for token in doc:\n            if not token.is_stop and not token.is_punct and not token.is_space:\n                year_lemmas.append(token.lemma_.lower())\n\n    counts = Counter(year_lemmas)\n    return counts.most_common(n)\n\nget_most_common_words(sou, 2024, n=10)\n\n","type":"content","url":"/nlp-p02#id-2-3-1-common-words-per-year-function","position":17},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.3.2 Compare 2023 to 2017","lvl2":"2.3 Common Words"},"type":"lvl3","url":"/nlp-p02#id-2-3-2-compare-2023-to-2017","position":18},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.3.2 Compare 2023 to 2017","lvl2":"2.3 Common Words"},"content":"\n\nwords_2023 = get_most_common_words(sou, 2023, n=20)\nwords_2017 = get_most_common_words(sou, 2017, n=20)\n\nwords_2023\n\nwords_2017\n\ndf_2023 = pd.DataFrame(words_2023, columns=[\"Word\", \"Count\"])\ndf_2017 = pd.DataFrame(words_2017, columns=[\"Word\", \"Count\"])\n\nfig, axes = plt.subplots(2, 1, figsize=(12, 12))\n\nsns.barplot(ax=axes[0], data=df_2017, x=\"Word\", y=\"Count\")\naxes[0].set_title(\"2017 State of the Union Most Frequent Words\")\naxes[0].tick_params(axis='x', rotation=90)\n\nsns.barplot(ax=axes[1], data=df_2023, x=\"Word\", y=\"Count\")\naxes[1].set_title(\"2023 State of the Union Most Frequent Words\")\naxes[1].tick_params(axis='x', rotation=90)\n\nplt.tight_layout()\nplt.show()\n\nplt.savefig(\"outputs/P02_figure_1.png\")\n\n","type":"content","url":"/nlp-p02#id-2-3-2-compare-2023-to-2017","position":19},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.4 TF-IDF Vectorization"},"type":"lvl2","url":"/nlp-p02#id-2-4-tf-idf-vectorization","position":20},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl2":"2.4 TF-IDF Vectorization"},"content":"We fit a TF-IDF vectorizer, plot all the speeches on a 2-D grid using PCA and also using a heatmap, and examine TF-IDF scores for the top 10 most common words in the first speech.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\n\n","type":"content","url":"/nlp-p02#id-2-4-tf-idf-vectorization","position":21},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.1 Train the Vectorizer and Transform the Data","lvl2":"2.4 TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#id-2-4-1-train-the-vectorizer-and-transform-the-data","position":22},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.1 Train the Vectorizer and Transform the Data","lvl2":"2.4 TF-IDF Vectorization"},"content":"\n\nraw_docs = sou[\"Text\"].to_list()\n\nvectorizer = TfidfVectorizer()\nvectorized_docs = vectorizer.fit_transform(raw_docs)\n\n","type":"content","url":"/nlp-p02#id-2-4-1-train-the-vectorizer-and-transform-the-data","position":23},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.2 Plot Speeches","lvl2":"2.4 TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#id-2-4-2-plot-speeches","position":24},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.2 Plot Speeches","lvl2":"2.4 TF-IDF Vectorization"},"content":"\n\npca = PCA(n_components=2)\nprincipal_components = pca.fit_transform(vectorized_docs.toarray())\npca_df = pd.DataFrame(data = principal_components, \n                      columns = ['Principle Component 1', 'Principle Component 2'])\n\nplt.figure(figsize=(10,6))\nplt.scatter(reduced[:,0], reduced[:,1])\nplt.title(\"Plot of Vectorized Speeches Principal Components\")\nplt.xlabel(\"Principal Component 1\")\nplt.ylabel(\"Principal Component 2\")\nplt.show()\n\nplt.savefig(\"outputs/P02_figure_2.png\")\n\ndense_vectorized_docs = vectorized_docs.toarray()\nplt.figure(figsize=(10, 10))\nsns.heatmap(\n    dense_vectorized_docs,\n    cmap= \"rocket\", \n    norm= LogNorm(vmin=dense_vectorized_docs[dense_vectorized_docs > 0].min(), \n                 vmax=dense_vectorized_docs.max())\n)\nplt.xticks(\n    ticks=np.arange(0, dense_vectorized_docs.shape[1], 928),\n    labels=np.arange(0, dense_vectorized_docs.shape[1], 928),\n    rotation=90\n)\n\nplt.yticks(\n    ticks=np.arange(0, dense_vectorized_docs.shape[0], 10),\n    labels=np.arange(0, dense_vectorized_docs.shape[0], 10)\n)\n\nplt.title(\"Vectorized Speeches\")\nplt.xlabel(\"Vector Index\")\nplt.ylabel(\"Speech Index\")\nplt.show()\n\nplt.savefig(\"outputs/P02_figure_3.png\")\n\n","type":"content","url":"/nlp-p02#id-2-4-2-plot-speeches","position":25},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.3 Get the TF-IDF value","lvl2":"2.4 TF-IDF Vectorization"},"type":"lvl3","url":"/nlp-p02#id-2-4-3-get-the-tf-idf-value","position":26},{"hierarchy":{"lvl1":"Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization (20 pts)","lvl3":"2.4.3 Get the TF-IDF value","lvl2":"2.4 TF-IDF Vectorization"},"content":"\n\nword_list = ['year','america','people','american','work','new','job','country','americans','world']\n\nword_nums = [vectorizer.vocabulary_.get(w, None) for w in word_list]\n\nidf_score = [vectorizer.idf_[i] for i in word_nums]\n\ntf_idf = [dense_matrix[0][i] for i in word_nums]\n\npd.DataFrame({\"Word\": word_list,\n              \"IDF Score\": idf_score,\n              \"TF-IDF Score\": tf_idf})","type":"content","url":"/nlp-p02#id-2-4-3-get-the-tf-idf-value","position":27},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"type":"lvl1","url":"/nlp-p03","position":0},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)"},"content":"","type":"content","url":"/nlp-p03","position":1},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Basic Imports"},"type":"lvl2","url":"/nlp-p03#basic-imports","position":2},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"Basic Imports"},"content":"\n\nfrom spacy import displacy\nfrom bertopic import BERTopic\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pyLDAvis\nimport pyLDAvis.gensim_models\nimport numpy as np\n\n","type":"content","url":"/nlp-p03#basic-imports","position":3},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.1 Preprocessing for Topic Modeling"},"type":"lvl2","url":"/nlp-p03#id-3-1-preprocessing-for-topic-modeling","position":4},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.1 Preprocessing for Topic Modeling"},"content":"We load the dataset, apply spaCy to tokenize and lemmatize text, and construct a list of lemma tokens for each document. Stop words, punctuation, and whitespace tokens are removed to prepare clean inputs for topic modeling.\n\nnlp = spacy.load(\"en_core_web_sm\")\nsou = pd.read_csv(\"data/SOTU.csv\")\n\ndef preprocess_text(text): \n    doc = nlp(text) \n    return [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and not token.is_space and len(token.lemma_) > 3]\n\nprocessed_docs = sou['Text'].apply(preprocess_text)\n\n","type":"content","url":"/nlp-p03#id-3-1-preprocessing-for-topic-modeling","position":5},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.2 LDA Topic Modeling"},"type":"lvl2","url":"/nlp-p03#id-3-2-lda-topic-modeling","position":6},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.2 LDA Topic Modeling"},"content":"We construct a dictionary and bag-of-words corpus from the lemma lists.\nUsing Gensim’s LDA model, we extract a set of topics from the entire SOTU speech collection.\n\ndictionary = Dictionary(processed_docs)\ndictionary.filter_extremes(no_below=5, no_above=0.5)\ncorpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n\nlda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=18, random_state=42, passes=10)\n\nlda_topics = lda.print_topics(num_words=10)\nprint(\"---LDA Topics---\")\nfor idx, topic in lda_model.print_topics(-1): \n    print(f\"Topic: {idx} \\nWords: {topic}\\n\")\n\n# print the topic distribution for the first speech\nfirst_doc_bow = corpus[0]\nfirst_doc_topics = lda.get_document_topics(first_doc_bow)\n\nprint(first_doc_topics)\n\npyLDAvis.enable_notebook()\n\nlda_vis = pyLDAvis.gensim_models.prepare(lda_model, corpus, dictionary)\nlda_vis\n\n","type":"content","url":"/nlp-p03#id-3-2-lda-topic-modeling","position":7},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"LDA Topic Interpretation"},"type":"lvl2","url":"/nlp-p03#lda-topic-interpretation","position":8},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"LDA Topic Interpretation"},"content":"The LDA topics generally capture high-frequency, surface-level themes in the corpus.\nCommon words such as government, year, world, nation, states, congress appear across many topics, showing that LDA groups speeches based on repeated vocabulary patterns rather than semantic meaning.\nBecause SOTU speeches share strong stylistic overlap, the LDA topics tend to be broad and partially overlapping.\n\n","type":"content","url":"/nlp-p03#lda-topic-interpretation","position":9},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.3 BERTopic Modeling"},"type":"lvl2","url":"/nlp-p03#id-3-3-bertopic-modeling","position":10},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.3 BERTopic Modeling"},"content":"We further apply BERTopic, which uses transformer embeddings and density-based clustering to form semantically coherent clusters.\n\ndocs = sou[\"Text\"].to_list()\n\nimport umap\n\numap_model = umap.UMAP(random_state=42)\n\ntopic_model = BERTopic(min_topic_size=3, umap_model=umap_model, calculate_probabilities=True)\ntopics, probs = topic_model.fit_transform(docs)\n\nvectorizer_model = CountVectorizer(stop_words=\"english\")\ntopic_model.update_topics(docs, vectorizer_model=vectorizer_model)\n\ntopic_info = topic_model.get_topic_info()\ntopic_info\n\nfirst_probs = probs[0]\ntopic_model.visualize_distribution(first_probs)\n\ntopic_model.visualize_topics()\n\n","type":"content","url":"/nlp-p03#id-3-3-bertopic-modeling","position":11},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"BERTopic Topic Interpretation"},"type":"lvl2","url":"/nlp-p03#bertopic-topic-interpretation","position":12},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"BERTopic Topic Interpretation"},"content":"BERTopic typically produces fewer, more semantically distinct topics.\nBecause transformer embeddings capture contextual meaning, the model groups speeches into broader conceptual clusters such as:\n\nGovernment/Congress/National Affairs\n\nAmerica/People/World/Future Themes\n\nThis reflects semantic coherence, contrasting with LDA’s frequency-driven clustering.\n\n","type":"content","url":"/nlp-p03#bertopic-topic-interpretation","position":13},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.4 Comparison: LDA vs BERTopic"},"type":"lvl2","url":"/nlp-p03#id-3-4-comparison-lda-vs-bertopic","position":14},{"hierarchy":{"lvl1":"Part 3: Advanced Text Processing - LDA and BERTopic Topic Modeling (20 pts)","lvl2":"3.4 Comparison: LDA vs BERTopic"},"content":"LDA uses a bag-of-words representation and identifies topics based on word co-occurrence.\n→ Produces several overlapping topics with similar vocabulary.\n\nBERTopic uses sentence-level semantic embeddings from transformers.\n→ Produces fewer, but more meaningful and interpretable clusters.\n\nSince SOTU speeches share structure and vocabulary,\n→ LDA tends to fragment content into many similar topics,\n→ BERTopic groups documents by broader themes reflecting actual meaning.\n\nOverall, BERTopic provides more coherent high-level topics, while LDA gives granular but noisy lexical clusters.","type":"content","url":"/nlp-p03#id-3-4-comparison-lda-vs-bertopic","position":15},{"hierarchy":{"lvl1":"Part 4: Choose your own adventure! (7 Points; Optional for Extra Credit)"},"type":"lvl1","url":"/nlp-p04","position":0},{"hierarchy":{"lvl1":"Part 4: Choose your own adventure! (7 Points; Optional for Extra Credit)"},"content":"Instruction:Please also make sure to structure your notebooks as if you were conducting this as a clean and nicely presented data analysis report. Do not include our prompts/problem statements in the final report notebooks.","type":"content","url":"/nlp-p04","position":1}]}